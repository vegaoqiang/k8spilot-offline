
# - name: Transfor ConfigMap To Control Plane
#   ansible.builtin.template:
#     src: tigera-configmap.yaml.j2
#     dest: /tmp/tigera-configmap.yaml


# - name: Apply Tigera ConfigMap
#   ansible.builtin.shell: |
#     kubectl create namespace tigera-operator
#     kubectl apply -f /tmp/tigera-configmap.yaml
#     rm -f /tmp/tigera-configmap.yaml

# - name: Set Architecture Alias
#   set_fact:
#     arch_alias: >-
#       {% if ansible_architecture == 'x86_64' %}amd64{% elif ansible_architecture == 'aarch64' %}arm64{% else %}{{ ansible_architecture }}{% endif %}

- name: Downloading k8spilot Job Image
  amazon.aws.s3_object:
    endpoint_url: "{{ s3_endpoint_url }}"
    access_key: "{{ s3_access_key }}"
    secret_key: "{{ s3_secret_key }}"
    bucket: "{{ s3_bucket_name }}"
    object: "pilot/pilot-job-v1.1-{{ arch_alias }}.tar"
    dest: "{{ playbook_dir }}/.ansible_temp/pilot-job-v1.1-{{ arch_alias }}.tar"
    mode: get
    overwrite: different
  delegate_to: localhost
  run_once: true
  retries: 5
  delay: 10

- name: Copy k8spilot Artifact To Remote Host
  ansible.builtin.copy:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
  loop:
    - { src: "{{ playbook_dir }}/.ansible_temp/pilot-job-v1.1-{{ arch_alias }}.tar", dest:  "/tmp/pilot-job-v1.1-{{ arch_alias }}.tar" }
    - { src: "pilot-job.yaml", dest: "{{ kubernetes_home}}/etc/pilot-job.yaml" }

- name: Import k8spilot Job Image To Containerd
  ansible.builtin.shell: ctr -n k8s.io i import /tmp/pilot-job-v1.1-{{ arch_alias }}.tar

- name: Deploy k8spilot Job
  ansible.builtin.shell: kubectl apply -f "{{ kubernetes_home}}/etc/pilot-job.yaml"

- name: Downloading tigera-operator Helm Chart
  amazon.aws.s3_object:
    endpoint_url: "{{ s3_endpoint_url }}"
    access_key: "{{ s3_access_key }}"
    secret_key: "{{ s3_secret_key }}"
    bucket: "{{ s3_bucket_name }}"
    object: "tigera-operator/tigera-operator-{{ calico_version }}.tgz"
    dest: "{{ playbook_dir }}/.ansible_temp/tigera-operator-{{ calico_version }}.tgz"
    mode: get
    overwrite: different
  delegate_to: localhost
  run_once: true

- name: Copy tigera-operator Helm Chart To Remote Host
  ansible.builtin.copy:
    src: "{{ playbook_dir }}/.ansible_temp/tigera-operator-{{ calico_version }}.tgz"
    dest: "/tmp/tigera-operator-{{ calico_version }}.tgz"

- name: Install Calico
  kubernetes.core.helm:
    chart_ref: "/tmp/tigera-operator-{{ calico_version }}.tgz"
    name: calico
    chart_version: "{{ calico_version }}"
    wait: false
    # state: absent
    release_namespace: tigera-operator
    create_namespace: true
    values:
      installation:
        calicoNetwork:
          linuxDataplane: BPF
          bgp: Disabled
          ipPools:
            - encapsulation: VXLANCrossSubnet
              cidr: "{{ kube_pod_ip_range }}"
        registry: "{{ calico_installation_registry }}"
        controlPlaneReplicas: 1
        # calicoNodeDaemonSet:
        #   properties:
        #     spec:
        #       properties:
        #         tolerations:
        #           items:
        #             properties:
        #               - effect: NoSchedule
        #                 operator: Exists
        #                 key: node.k8spilot.io/network-not-ready
      apiServer:
        enabled: false
      goldmane:
        enabled: false
      whisker:
        enabled: false
      kubernetesServiceEndpoint:
        host: "{{ ansible_default_ipv4.address }}"
        port: "6443"
      kubeletVolumePluginPath: None
      tolerations:
        - effect: NoSchedule
          operator: Exists
          key: node.k8spilot.io/network-not-ready

# - name: Wait k8spilot pod Running
#   ansible.builtin.shell: kubectl get pod -n kube-system -l job-name=pilot-job -o json
#   register: pod_result
  # until: pod_result.output|from_json|get('items', None)
  # retries: 60
  # delay: 5

# - name: debug
#   ansible.builtin.debug:
#     msg: "{{ pod_result.stdout | from_json }}"